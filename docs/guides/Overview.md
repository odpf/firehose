
#### Create a Log Sink
**A log sink firehose requires the following variables to be set.**

| VARIABLE NAME | DESCRIPTION | NECESSITY | SAMPLE VALUE |
|---|---|---|---|
| source.kafka.brokers | list of kafka brokers to consume from  | Mandatory | localhost:9092 |
| source.kafka.topic | list of kafka topics to consume from  | Mandatory | test-topic |
| source.kafka.consumer.group.id | Kafka consumer group ID | Mandatory | sample-group-id |
| kafka.record.parcer.mode | Decides whether to parse key or message | Mandatory | message |
| sink.type | Firehose sink type | Mandatory | log |
| input.schema.proto.class | The fully qualified name of the input proto class | Mandatory | com.tests.TestMessage |

#### Create an HTTP Sink
**Data read from Kafka is written to an HTTP endpoint and it requires the following variables to be set. You need to create your own HTTP endpoint so that the Firehose can send data to it.**

| VARIABLE NAME                                                       | DESCRIPTION                                                                                                                                                                                                                                                                                                                                                                                                                          | NECESSITY                                            | SAMPLE VALUE                                                                                                                                                                                                                             |
| ------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| sink.http.service.url                                               | The HTTP endpoint of the service to which this consumer should PUT/POST data. This can be configured as per the requirement, a constant or a dynamic one (which extract given field values from each message and use that as the endpoint)<br>If service url is constant, messages will be sent as batches while in case of dynamic one each message will be sent as a separate request (Since they’d be having different endpoints) | Mandatory                                            | http://http-service.test.io<br><br>http://http-service.test.io/test-field/%%s,6<br><br>(This will take the value with index 6 from proto and create the endpoint as per the template) NOTE: need to use %% instead of single % |
| sink.http.request.method                                            | HTTP verb supported by the endpoint, Supports PUT, and POST verbs as of now.<br>Default value: put                                                                                                                                                                                                                                                                                                                                   | Mandatory                                            | post                                                                                                                                                                                                                                     |
| sink.http.request.timeout.ms                                        | The connection timeout for the request.<br>Default value: 10000                                                                                                                                                                                                                                                                                                                                                                      | Mandatory                                            | 10000                                                                                                                                                                                                                                    |
| sink.http.max.connections                                           | The maximum number of HTTP connections.<br>Default value: 10                                                                                                                                                                                                                                                                                                                                                                         | Mandatory                                            | 10                                                                                                                                                                                                                                       |
| sink.http.retry.status.code.ranges                                  | The range of HTTP status codes for which retry will be attempted.<br>Default value: 400-600                                                                                                                                                                                                                                                                                                                                          | Optional                                             | 400-600                                                                                                                                                                                                                                  |
| sink.http.data.format                                               | If proto, the log message will be sent as Protobuf byte strings. Otherwise, the log message will be deserialized into readable JSON strings.<br>Default value: proto                                                                                                                                                                                                                                                                 | Mandatory                                            | JSON                                                                                                                                                                                                                                     |
| sink.http.headers                                                   | The HTTP headers required to push the data to the above URL.                                                                                                                                                                                                                                                                                                                                                                         | Optional                                             | Authorization:auth\_token, Accept:text/plain                                                                                                                                                                                             |
| sink.http.json.body.template                                        | A template for creating a custom request body from the fields of a protobuf message. This should be a valid JSON itself.                                                                                                                                                                                                                                                                                                             | Optional                                             | {"test":"$.routes\[0\]", "$.order\_number" : "xxx"}                                                                                                                                                                                      |
| sink.http.parameter.source                                          | The source from which the fields should be parsed. This field should be present in order to use this feature.<br>Parameterized sink-specific<br>Default value: None                                                                                                                                                                                                                                                                  | Optional                                             | None, Key, Message                                                                                                                                                                                                                       |
| sink.http.parameter.placement                                       | The fields parsed can be passed in query parameters or in headers.<br>Parameterized sink-specific                                                                                                                                                                                                                                                                                                                                    | Optional                                             | Header, Query                                                                                                                                                                                                                            |
| input.output.mapping                                                | The mapping of the proto fields to header/query fields in JSON format.<br>Parameterized sink-specific                                                                                                                                                                                                                                                                                                                                | Optional                                             | {"1":"order\_number","2":"event\_timestamp","3":"driver\_id"}                                                                                                                                                                            |
| sink.http.oauth2.enable                                             | Enable/Disable OAuth2 support for HTTP sink<br>Default value: false                                                                                                                                                                                                                                                                                                                                                                  | Optional                                             | FALSE                                                                                                                                                                                                                                    |
| sink.http.oauth2.access.token.url                                   | OAuth2 Token Endpoint.                                                                                                                                                                                                                                                                                                                                                                                                               | Mandatory<br>( when sink.http.oauth2.enable = true ) | https://sample-oauth.my-api.com/oauth2/token                                                                                                                            |
| sink.http.oauth2.client.name                                        | OAuth2 identifier issued to the client.                                                                                                                                                                                                                                                                                                                                                                                              | Mandatory<br>( when sink.http.oauth2.enable = true ) | clientid                                                                                                                                                                                                                                 |
| sink.http.oauth2.client.secret                                      | OAuth2 secret issued for the client.                                                                                                                                                                                                                                                                                                                                                                                                 | Mandatory<br>( when sink.http.oauth2.enable = true ) | my-secret                                                                                                                                                                                                                                |
| sink.http.oauth2.scope                                              | Space-delimited scope overrides. If scope override is not provided, no scopes will be granted to the token.                                                                                                                                                                                                                                                                                                                          | Mandatory<br>( when sink.http.oauth2.enable = true ) | User:read, sys:info                                                                                                                                                                                                                      |

**Usage of sink.http.json.body.template is explained here.** 

***Templating Body In Firehose***
We are using: https://github.com/json-path/JsonPath - for creating Templates which is a DSL for basic JSON parsing. Playground for this: https://jsonpath.com/, where users can play around with a given JSON to extract out the elements as required and validate the jsonpath. The template works only when the output data format `sink.http.data.format` is JSON.

***Creating Templates:***

This is really simple. Find the paths you need to extract using the JSON path. Create a valid JSON template with the static field names + the paths that need to extract. (Paths name starts with $.). Firehose will simply replace the paths with the actual data in the path of the message accordingly. Paths can also be used on keys, but be careful that the element in the key must be a string data type.

One sample configuration : {"test":"$.routes[0]", "$.order_number" : "xxx"} (On XYZ proto).
If you want to dump the entire JSON as it is in the backend, use "$._all_" as a path.

Limitations:
* Works when the input DATA TYPE is a protobuf, not a JSON.
* Supports only on messages, not keys.
* validation on the level of valid JSON template. But after data has been replaced the resulting string may or may not be a valid JSON. Users must do proper testing/validation from the service side.
* If selecting fields from complex data type like repeated/messages/map of proto, the user must do filtering based first as selecting a field that does not exist would fail.


#### Create a JDBC SINK
* Supports only PostgresDB as of now
* Data read from Kafka is written to the PostgresDB database and it requires the following variables to be set.

| VARIABLE NAME                             | DESCRIPTION                                                                                                                 | NECESSITY | SAMPLE VALUE                                                                                                                                                           |
| ----------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- | --------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| sink.jdbc.url                             | PostgresDB URL, it's usually the hostname followed by port.                                                                 | Mandatory | jdbc:postgresql://localhost:5432/postgres                                                                                                                 |
| sink.jdbc.table.name                      | The name of the table in which the data should be dumped.                                                                   | Mandatory | public.customers                                                                                                                                                       |
| sink.jdbc.username                        | The username to connect to DB                                                                                               | Mandatory | root                                                                                                                                                                   |
| sink.jdbc.password                        | The password to connect to DB                                                                                               | Mandatory | root                                                                                                                                                                   |
| input.schema.proto.to.column.mapping      | The mapping of fields in DB and the corresponding proto index from where the value will be extracted. This is a JSON field. | Mandatory | {"6":"customer\_id","1":"service\_type","5":"event\_timestamp"}(Proto field value with index 1 will be stored in a column named service\_type in DB and so on)         |
| sink.jdbc.unique.keys                     | Comma-separated column names having a unique constraint on the table.<br>Default value: NONE                                    | Optional  | customer\_id                                                                                                                                                           |
| sink.jdbc.connection.pool.timeout.ms      | Database connection timeout in milliseconds.<br>Default value: 1000                                                             | Mandatory | 2000                                                                                                                                                                   |
| sink.jdbc.connection.pool.idle.timeout.ms | Database connection pool idle connection timeout in milliseconds.<br>Default value: 60000                                       | Mandatory | 30000                                                                                                                                                                  |
| sink.jdbc.connection.pool.min.idle        | The minimum number of idle connections in the pool to maintain.<br>Default value: 0                                             | Mandatory | 0                                                                                                                                                                      |
| sink.jdbc.connection.pool.max.size        | Maximum size for the database connection pool.<br>Default value: 10                                                           | Mandatory | 10                                                                                                                                                                     |

***Note: Schema (Table, Columns, and Any Constraints) being used in firehose configuration must exist in the Database already.***

#### Create an Influx Sink
* Data read from Kafka is written to the InfluxDB time-series database and it requires the following variables to be set.

| VARIABLE NAME                              | DESCRIPTION                                                                                                                                                                                                                                                                                                                                                                         | NECESSITY | SAMPLE VALUE                                                                                                                                                         |
| ------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| sink.influx.url                            | InfluxDB URL, it's usually the hostname followed by port.                                                                                                                                                                                                                                                                                                                           | Mandatory | http://localhost:8086                                                                                                                                                |
| sink.influx.username                       | The username to connect to DB                                                                                                                                                                                                                                                                                                                                                       | Mandatory | root                                                                                                                                                                 |
| sink.influx.password                       | The password to connect to DB                                                                                                                                                                                                                                                                                                                                                       | Mandatory | root                                                                                                                                                                 |
| sink.influx.field.name.proto.index.mapping | The mapping of fields and the corresponding proto index which can be used to extract the field value from the proto message. This is a JSON field. Note that Influx keeps a single value for each unique set of tags and timestamps. If a new value comes with the same tag and timestamp from the source, it will override the existing one.                                       | Mandatory | {"2":"order\_number","1":"service\_type", "4":"status"<br>(Proto field value with index 2 will be stored in a field named 'order\_numbert' in Influx and so on) |
| sink.influx.tag.name.proto.index.mapping   | The mapping of tags and the corresponding proto index from which the value for the tags can be obtained. If the tags contain existing fields from field name mapping it will not be overridden. They will be duplicated. If ENUMS are present then they must be added here. This is a JSON field.                                                                                   | Optional  | {"6":"customer\_id"}<br>                                                                                                                                           |
| sink.influx.proto.event.timestamp.index    | Proto index of a field that can be used as the timestamp                                                                                                                                                                                                                                                                                                                            | Mandatory | 5                                                                                                                                                                    |
| sink.influx.db.name                        | InfluxDB database name where data will be dumped                                                                                                                                                                                                                                                                                                                                    | Mandatory | status                                                                                                                                                               |
| sink.influx.measurement.name               | This field is used to give away the name of the measurement that needs to be used by the sink. Measurement is another name for tables and it will be auto-created if not exist at the time Firehose pushes the data to the influx.                                                                                                                                                  | Mandatory | customer-booking                                                                                                                                                     |
| sink.influx.retention.policy               | Retention policy for influx database.<br>Default value: autogen                                                                                                                                                                                                                                                                                                                     | Mandatory | quarterly                                                                                                                                                            |

***Note: DATABASE and RETENTION POLICY being used in firehose configuration must exist already in the Influx, It’s outside the scope of a firehose and won’t be generated automatically.***

#### Create a Redis Sink

* Redis sink can be created in 2 different modes based on the value of `sink.redis.data.type`: Hashset or List
    * Hashset: For each message, an entry of the format ‘key : field : value’ is generated and pushed to Redis. field and value are generated on the basis of the config `input.schema.proto.to.column.mapping`
    * List: For each message entry of the format ‘key : value’ is generated and pushed to Redis. Value is fetched for the proto index provided in the config `sink.redis.list.data.proto.index`
* The ‘key’ is picked up from a field in the message itself.
* Limitation: Firehose Redis sink only supports HashSet and List entries as of now.
* it requires the following variables to be set.

| VARIABLE NAME                    | DESCRIPTION                                                                                                                                                                                                                   | NECESSITY              | SAMPLE VALUE                                                                                                                                                                                                             |
| -------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| sink.redis.urls                  | REDIS instance hostname/IP address followed by its port                                                                                                                                                                       | Mandatory              | localhos:6379,localhost:6380                                                                                                                                                                                                             |
| sink.redis.data.type             | To select whether you want to push your data as a HashSet or as a List.Default value: LIST                                                                                                                                    | Mandatory              | List / Hashset                                                                                                                                                                                                             |
| sink.redis.key.template          | The string that will act as the key for each Redis entry. This key can be configured as per the requirement, a constant or can extract value from each message and use that as the Redis key. <br>                            | Mandatory              | Service\_%%s,1<br>(This will take the value with index 1  from proto and create the Redis keys as per the template)                                                                                                        |
| input.schema.proto.to.column.mapping| This is the field that decides what all data will be stored in the HashSet for each message.                                                                                                                               | Mandatory<br>(Hashset) | {"6":"customer\_id",  "2":"order\_num"}                                                                                                                                                                                 |
| sink.redis.list.data.proto.index | This field decides what all data will be stored in the List for each message. <br>                                                                                                                                            | Mandatory<br>(List)     | 6<br>(this will get the value of the field with index 6 in your proto and push that to the Redis list with the corresponding keyTemplate)                                                                                 |
| sink.redis.ttl.type              | Choice of Redis TTL type.It can be:<br>DURATION: After which the Key will be expired and removed from Redis (UNIT- seconds)<br>EXACT\_TIME: Precise UNIX timestamp after which the Key will be expired<br>Default value: DISABLE | Optional               | DISABLE / DURATION / EXACT\_TIME                                                                                                                                                                                                 |
| sink.redis.ttl.value             | Redis TTL value in Unix Timestamp for EXACT\_TIME TTL type, In Seconds for DURATION TTL type<br>Default value: 0                                                                                                                  | Optional               | 100000<br>                                                                                                                                                                                                                 |
| sink.redis.deployment.type       | The Redis deployment you are using. At present, we support standalone and cluster types.<br>Default value: Standalone                                                                                                             | Optional               | Cluster                                                                                                                                                                                                                 |

#### Crete a Redis Cluster Sink

* Just like Redis sink, Redis cluster sink can be created in 2 different modes based on the value of `sink.redis.data.type`: Hashset or List
    * Hashset: For each message, an entry of the format ‘key : field : value’ is generated and pushed to Redis. field and value are generated on the basis of the config `input.schema.proto.to.column.mapping`
    * List: For each message, an entry of the format ‘key : value’ is generated and pushed to Redis. Value is fetched for the proto index provided in the config `sink.redis.list.data.proto.index`
* The ‘key’ is picked up from a field in the message itself.
* Limitation: Firehose Redis sink only supports HashSet and List entries as of now.
* it requires the following variables to be set.

| VARIABLE NAME                    | DESCRIPTION                                                                                                                                                                                                                   | NECESSITY              | SAMPLE VALUE                                                                                                                                                                                                             |
| -------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| sink.redis.urls                  | REDIS instance hostname/IP address followed by its port                                                                                                                                                                       | Mandatory              | localhost:6379                                                                                                                                                                                                             |
| sink.redis.data.type             | To select whether you want to push your data as a HashSet or as a List.Default value: LIST                                                                                                                                    | Mandatory              | List / Hashset                                                                                                                                                                                                             |
| sink.redis.key.template          | The string that will act as the key for each Redis entry. This key can be configured as per the requirement, a constant or can extract value from each message and use that as the Redis key. <br>                            | Mandatory              | Service\_%%s,1<br>(This will take the value with index 1  from proto and create the Redis keys as per the template)                                                                                                        |
| input.schema.proto.to.column.mapping| This is the field that decides what all data will be stored in the HashSet for each message.                                                                                                                               | Mandatory<br>(Hashset) | {"6":"customer\_id",  "2":"order\_num"}                                                                                                                                                                                 |
| sink.redis.list.data.proto.index | This field decides what all data will be stored in the List for each message. <br>                                                                                                                                            | Mandatory<br>(List)     | 6<br>(this will get the value of the field with index 6 in your proto and push that to the Redis list with the corresponding keyTemplate)                                                                                 |
| sink.redis.ttl.type              | Choice of Redis TTL type.It can be:<br>DURATION: After which the Key will be expired and removed from Redis (UNIT- seconds)<br>EXACT\_TIME: Precise UNIX timestamp after which the Key will be expired<br>Default value: DISABLE | Optional               | DISABLE / DURATION / EXACT\_TIME                                                                                                                                                                                                 |
| sink.redis.ttl.value             | Redis TTL value in Unix Timestamp for EXACT\_TIME TTL type, In Seconds for DURATION TTL type<br>Default value: 0                                                                                                                  | Optional               | 100000<br>                                                                                                                                                                                                                 |
| sink.redis.deployment.type       | The Redis deployment you are using. At present, we support standalone and cluster types.<br>Default value: Standalone                                                                                                             | Optional               | Standalone                                                                                                                                                                                                                 |

#### Create an ElasticSearch Sink

* In the ElasticSearch sink, each message is converted into a document in the specified index with the Document type and ID as specified by the user
* ElasticSearch sink supports reading messages in both JSON and Protobuf formats.
* it requires the following variables to be set.

| VARIABLE NAME                             | DESCRIPTION                                                                                                                                                                                                                                                                                                                                                                  | NECESSITY | SAMPLE VALUE    |
| ----------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | --------------- |
| sink.es.connection.urls                   | Elastic search connection URL to connect. URLs could be comma separated like host\_ip1:port1, host\_ip2:port2                                                                                                                                                                                                                                                                | Mandatory | localhost1:9200 |
| sink.es.index.name                        | The name of the index to which you want to write the documents. If it doesn’t exist, it will be created.                                                                                                                                                                                                                                                                     | Mandatory | sample\_index   |
| sink.es.type.name                         | The type name of the Document in ElasticSearch.                                                                                                                                                                                                                                                                                                                              | Mandatory | Customer        |
| sink.es.id.field                          | The identifier field of the document in ElasticSearch. This should be the key of the field present in the message (JSON or Protobuf) and it has to be a unique, non-null field. So the value of this field in the message will be used as the ID of the document in ElasticSearch while writing the document.                                                                | Mandatory | customer\_id    |
| sink.es.mode.update.only.enable           | Elasticsearch sink can be created in 2 modes: Upsert mode or UpdateOnly mode<br>If this config is set to-<br>TRUE: Firehose will run on UpdateOnly mode which will only UPDATE the already existing documents in the ElasticSearch index.<br>FALSE: Firehose will run on Upsert mode, UPDATING the existing documents and also INSERTING any new ones<br>Default value: FALSE| Mandatory | FALSE           |
| sink.es.input.message.type                | Indicates if the Kafka topic contains JSON or Protocol Buffer messages.<br>Default value: JSON                                                                                                                                                                                                                                                                               | Mandatory | PROTOBUF        |
| sink.es.preserve.proto.field.names.enable | Whether or not the protobuf field names should be preserved in the ElasticSearch document. If false the fields will be converted to camel case.<br>Default value: TRUE                                                                                                                                                                                                             | Mandatory | FALSE           |
| sink.es.request.timeout.ms                | The request timeout of the elastic search endpoint. The value specified is in milliseconds<br>Default value: 60000                                                                                                                                                                                                                                                           | Mandatory | 60000           |
| sink.es.shards.active.wait.count          | The number of shard copies that must be active before proceeding with the operation. This can be set to any non-negative value less than or equal to the total number of shard copies (number of replicas + 1)<br>Default value: 1                                                                                                                                           | Mandatory | 1               |
| sink.es.retry.status.code.blacklist       | List of comma-separated status codes for which firehose should not retry in case of UPDATE ONLY mode is TRUE<br>Default value: 404                                                                                                                                                                                                                                           | Optional  | 404,400         |
| sink.es.routing.key.name                  | The proto field whose value will be used for routing documents to a particular shard in Elasticsearch. If empty, Elasticsearch uses the ID field of the doc by default                                                                                                                                                                                                       | Optional  | service\_type   |

#### Create a GRPC Sink

* Data read from Kafka is written to a GRPC endpoint and it requires the following variables to be set.
* You need to create your own GRPC endpoint so that the Firehose can send data to it. The response proto should have a field “success” with value as true or false.

| VARIABLE NAME                   | DESCRIPTION                                                | NECESSITY | SAMPLE VALUE                                                                                            |
| ------------------------------- | ---------------------------------------------------------- | --------- | ---------------------------------------- |
| sink.grpc.service.host          | The host of the GRPC service                               | Mandatory | http://grpc-service.sample.io, 127.0.0.1 |
| sink.grpc.service.port          | Port of the GRPC service                                   | Mandatory | 8500                                     |
| sink.grpc.method.url            | URL of the GRPC method that needs to be called             | Mandatory | com.tests.SampleServer/SomeMethod        |
| sink.grpc.response.proto.schema | Proto which would be the response of the GRPC Method       | Mandatory | Values derived from protos pre-defined   |

#### Define Standard Configurations

* These are the configurations that remain common across all the Sink Types.
* You don’t need to modify them necessarily, It is recommended to use them with the default values. Read their descriptions for more details.

| VARIABLE NAME                                 | DESCRIPTION                                                                                               | NECESSITY | SAMPLE VALUE |
| --------------------------------------------- | --------------------------------------------------------------------------------------------------------- | --------- | ------------ |
| source.kafka.consumer.config.max.poll.records | The maximum number of records, the consumer will fetch from Kafka in one request.<br>Default value: 500   | Mandatory | 500          |
| retry.exponential.backoff.initial.ms          | Initial expiry time in milliseconds for exponential backoff policy.<br>Default value: 10                  | Mandatory | 10           |
| retry.exponential.backoff.rate                | Backoff rate for exponential backoff policy.<br>Default value: 2                                          | Mandatory | 2            |
| retry.exponential.backoff.max.ms              | Maximum expiry time in milliseconds for exponential backoff policy.<br>Default value: 60000               | Mandatory | 60000        |

#### Filter Expressions
**Introduction**

Filter expressions are allowed to filter messages just after reading from Kafka and before sending to Sink.

##### Rules to write expressions:

* All the expressions are like a piece of Java code.
* Follow rules for every data type, as like writing a Java code.
* Access nested fields by `.` and `()`, i.e., `sampleLogMessage.getVehicleType()`

**Example**

Sample proto message:

```
===================KEY==========================
driver_id: "abcde12345"
vehicle_type: BIKE
event_timestamp {
  seconds: 186178
  nanos: 323080
}
driver_status: UNAVAILABLE

================= MESSAGE=======================
driver_id: "abcde12345"
vehicle_type: BIKE
event_timestamp {
  seconds: 186178
  nanos: 323080
}
driver_status: UNAVAILABLE
app_version: "1.0.0"
driver_location {
  latitude: 0.6487193703651428
  longitude: 0.791822075843811
  altitude_in_meters: 0.9949166178703308
  accuracy_in_meters: 0.39277541637420654
  speed_in_meters_per_second: 0.28804516792297363
}
gcm_key: "abc123"
```

***Key based filter expressions examples:***

* `sampleLogKey.getDriverId()=="abcde12345"`
* `sampleLogKey.getVehicleType()=="BIKE"`
* `sampleLogKey.getEventTimestamp().getSeconds()==186178`
* `sampleLogKey.getDriverId()=="abcde12345"&&sampleLogKey.getVehicleType=="BIKE"` (multiple conditions example 1)
* `sampleLogKey.getVehicleType()=="BIKE"||sampleLogKey.getEventTimestamp().getSeconds()==186178` (multiple conditions example 2)

***Message based filter expressions examples:***

* `sampleLogMessage.getGcmKey()=="abc123"`
* `sampleLogMessage.getDriverId()=="abcde12345"&&sampleLogMessage.getDriverLocation().getLatitude()>0.6487193703651428`
* `sampleLogMessage.getDriverLocation().getAltitudeInMeters>0.9949166178703308`

**Note: Use `sink.type=log` for testing the applied filtering** 
